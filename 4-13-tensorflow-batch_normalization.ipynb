{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>TensorFlow による実装</h2>\n",
       "<ul>\n",
       "  <li>early stopping 導入</li>\n",
       "  <li>batch normalization 導入</li>\n",
       "</ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h2>TensorFlow による実装</h2>\n",
    "<ul>\n",
    "  <li>early stopping 導入</li>\n",
    "  <li>batch normalization 導入</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.148417  accuracy: 0.965813\n",
      "epoch: 1  loss: 0.0782671  accuracy: 0.981125\n",
      "epoch: 2  loss: 0.0382322  accuracy: 0.991063\n",
      "epoch: 3  loss: 0.0292092  accuracy: 0.99275\n",
      "epoch: 4  loss: 0.020165  accuracy: 0.995562\n",
      "epoch: 5  loss: 0.0149883  accuracy: 0.996\n",
      "epoch: 6  loss: 0.0110507  accuracy: 0.997938\n",
      "epoch: 7  loss: 0.0113003  accuracy: 0.997312\n",
      "epoch: 8  loss: 0.0123916  accuracy: 0.996687\n",
      "epoch: 9  loss: 0.00852941  accuracy: 0.998062\n",
      "epoch: 10  loss: 0.0068584  accuracy: 0.998312\n",
      "epoch: 11  loss: 0.00566791  accuracy: 0.998812\n",
      "epoch: 12  loss: 0.00192533  accuracy: 0.999812\n",
      "epoch: 13  loss: 0.00411614  accuracy: 0.999188\n",
      "epoch: 14  loss: 0.00966534  accuracy: 0.997312\n",
      "epoch: 15  loss: 0.00469177  accuracy: 0.998937\n",
      "epoch: 16  loss: 0.00485709  accuracy: 0.998625\n",
      "early stopping\n",
      "accuracy:  0.97125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self._loss_min = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def validate(self, loss):\n",
    "        #if self._loss < loss:\n",
    "        if self._loss_min < loss:    \n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "            self._loss_min = loss\n",
    "\n",
    "        self._loss_min = min(loss, self._loss_min)\n",
    "        return False\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self._x = None\n",
    "        self._t = None,\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "        self.accurary_op = None\n",
    "        self.early_stopping = EarlyStopping(patience=4, verbose=1)\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # Batch normalization 処理\n",
    "    def batch_normalization(self, shape, x):\n",
    "        eps = 1e-8\n",
    "        beta = tf.Variable(tf.zeros(shape))\n",
    "        gamma = tf.Variable(tf.ones(shape))\n",
    "        mean, var = tf.nn.moments(x, [0])\n",
    "        return gamma * (x - mean) / tf.sqrt(var + eps) + beta\n",
    "    \n",
    "    def inference(self, x, keep_prob):\n",
    "        # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            #self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            #self.biases.append(self.bias_variable([n_hidden]))\n",
    "            #h = tf.nn.relu(tf.matmul(\n",
    "            #    input, self.weights[-1]) + self.biases[-1])\n",
    "            #output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "            W = self.weight_variable([input_dim, n_hidden])\n",
    "            u = tf.matmul(input, W)\n",
    "            h = self.batch_normalization([n_hidden], u)\n",
    "            output = tf.nn.relu(h)\n",
    "\n",
    "\n",
    "        # 隠れ層 - 出力層\n",
    "        \"\"\"\n",
    "        self.weights.append(\n",
    "            self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(\n",
    "            output, self.weights[-1]) + self.biases[-1])\n",
    "        \"\"\"\n",
    "        \n",
    "        W_out = self.weight_variable([self.n_hiddens[-1], self.n_out])\n",
    "        b_out = self.bias_variable([self.n_out])\n",
    "        y = tf.nn.softmax(tf.matmul(output, W_out) + b_out)\n",
    "    \n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(\n",
    "                       t * tf.log(tf.clip_by_value(y, 1e-10, 1.0)),\n",
    "                       reduction_indices=[1]))\n",
    "        return cross_entropy\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                       beta1=0.9, beta2=0.999)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train,\n",
    "            nb_epoch=100, batch_size=100, p_keep=0.5,\n",
    "            verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "        self.accurary_op = accuracy\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            if math.isnan(loss_):\n",
    "                break\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            \n",
    "            # Early Stopping チェック\n",
    "            if self.early_stopping.validate(loss_):\n",
    "                break\n",
    "            \n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                      ' loss:', loss_,\n",
    "                      ' accuracy:', accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        return self.accurary_op.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    データの生成\n",
    "    '''\n",
    "    mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "    n = len(mnist.data)\n",
    "    N = 20000  # MNISTの一部を使う\n",
    "    indices = np.random.permutation(range(n))[:N]  # ランダムにN枚を選択\n",
    "\n",
    "    X = mnist.data[indices]\n",
    "    y = mnist.target[indices]\n",
    "    Y = np.eye(10)[y.astype(int)]  # 1-of-K 表現に変換\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "        \n",
    "    '''\n",
    "    モデル設定\n",
    "    '''\n",
    "    model = DNN(n_in=len(X[0]),\n",
    "                n_hiddens=[200, 200, 200],\n",
    "                n_out=len(Y[0]))\n",
    "\n",
    "    '''\n",
    "    モデル学習\n",
    "    '''\n",
    "    model.fit(X_train, Y_train,\n",
    "              nb_epoch=60,\n",
    "              batch_size=200,\n",
    "              p_keep=0.5)\n",
    "\n",
    "    '''\n",
    "    予測精度の評価\n",
    "    '''\n",
    "    accuracy = model.evaluate(X_test, Y_test)\n",
    "    print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
